{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Tokenization Exercise\n",
    "\n",
    "This exercise explores the challenges of splitting text into sentences and words when dealing with complex real-world text containing dates, amounts, URLs, emails, acronyms, and multi-word expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge\n",
    "\n",
    "Given a text variable, split it into:\n",
    "1. **Sentences** - logical units of meaning ending with terminal punctuation\n",
    "2. **Words (tokens)** - individual meaningful units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Dr. John Smith, Ph.D., earned $1,250.50 on Jan. 15, 2024, for his work at A.I. Corp. You can reach him at j.smith@ai-corp.co.uk or visit https://www.ai-corp.co.uk/team/dr-smith for more info. The U.S.A.-based company reported a 23.5% increase in Q3 revenue, totaling €2.5M.\n",
      "Sentences:\n",
      "- Dr.\n",
      "- John Smith, Ph.D., earned $1,250.50 on Jan.\n",
      "- 15, 2024, for his work at A.I.\n",
      "- Corp.\n",
      "- You can reach him at j.smith@ai-corp.co.uk or visit https://www.ai-corp.co.uk/team/dr-smith for more info.\n",
      "- The U.S.A.-based company reported a 23.5% increase in Q3 revenue, totaling €2.5M.\n"
     ]
    }
   ],
   "source": [
    "# Sample text with challenging elements\n",
    "import re\n",
    "text = \"\"\"Dr. John Smith, Ph.D., earned $1,250.50 on Jan. 15, 2024, for his work at A.I. Corp. You can reach him at j.smith@ai-corp.co.uk or visit https://www.ai-corp.co.uk/team/dr-smith for more info. The U.S.A.-based company reported a 23.5% increase in Q3 revenue, totaling €2.5M.\"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "\n",
    "print(\"Sentences:\")\n",
    "for s in sentences:\n",
    "    print(\"-\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens:\n",
      "['Dr', 'John', 'Smith', 'Ph', 'D', 'earned', '1', '250', '50', 'on', 'Jan', '15', '2024', 'for', 'his', 'work', 'at', 'A', 'I', 'Corp', 'You', 'can', 'reach', 'him', 'at', 'j', 'smith', 'ai', 'corp', 'co', 'uk', 'or', 'visit', 'https', 'www', 'ai', 'corp', 'co', 'uk', 'team', 'dr', 'smith', 'for', 'more', 'info', 'The', 'U', 'S', 'A', 'based', 'company', 'reported', 'a', '23', '5', 'increase', 'in', 'Q3', 'revenue', 'totaling', '2', '5M']\n"
     ]
    }
   ],
   "source": [
    "tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "print(\"\\nTokens:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Tokenization Exercise\n",
    "\n",
    "This exercise explores the challenges of splitting words in large corpuses and find the most common words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge\n",
    "\n",
    "Given a file `shakes.txt` in the book folder. Find the words that are more common in Shakespeare's book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete for: C:\\Users\\Anxo\\Downloads\\shakes.txt\n",
      "------------------------------\n",
      "the: 27843\n",
      "and: 26847\n",
      "i: 22538\n",
      "to: 19883\n",
      "of: 18307\n",
      "a: 14800\n",
      "you: 13928\n",
      "my: 12489\n",
      "that: 11563\n",
      "in: 11183\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "file_path = r'C:\\Users\\Anxo\\Downloads\\shakes.txt'\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        # 1. Read and lowercase the text\n",
    "        content = f.read().lower()\n",
    "        \n",
    "        # 2. Extract only the words\n",
    "        words = re.findall(r'\\b\\w+\\b', content)\n",
    "        \n",
    "        # 3. Count the 10 most frequent words\n",
    "        top_words = Counter(words).most_common(10)\n",
    "\n",
    "    print(f\"Analysis complete for: {file_path}\")\n",
    "    print(\"-\" * 30)\n",
    "    for word, count in top_words:\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {file_path}. Check the filename!\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
